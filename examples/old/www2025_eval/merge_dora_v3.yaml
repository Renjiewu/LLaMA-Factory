model_name_or_path: Qwen/Qwen2-VL-7B-Instruct
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explain
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv2
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv2_banlence
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3_with_extra_data
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3_with_extra_data/checkpoint-800
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3_with_extra_datav2
adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3_with_extra_datav2/checkpoint-874
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3/checkpoint-399
# adapter_name_or_path: output/qwen2_vl-7b/dora/dora_r32_ep3_explainv3


### method
stage: sft
do_train: false
do_predict: true # 指定进行预测, 不训练
predict_with_generate: true
finetuning_type: lora

### dataset
# eval_dataset: mire_eval_split_img, mire_eval_split_dia  #修改为测试集
eval_dataset: mire_explain_enhance3_eval_img, mire_explain_enhance3_eval_dia  #修改为测试集
template: qwen2_vl
cutoff_len: 5125
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16 # 这个只会影响内存


### output
output_dir: output/qwen2_vl-7b/dora/sft-infer-dora_r32_ep3_explainv3-v2-874-2 #修改为保存地址
logging_steps: 20
overwrite_output_dir: true


### eval
per_device_eval_batch_size: 2  # A100显存充足，可以适当提高batch size
flash_attn: fa2

### generation
max_new_tokens: 128
do_sample: false
# num_beams: 3
# num_beam_groups: 3
# temperature: 0.5 # 不设置
# top_k: 3
# use_flash_attention_2: true    # 启用flash attention 2加速
# torch_dtype: bfloat16         # 使用bfloat16进行推理
use_cache: true              # 启用KV cache